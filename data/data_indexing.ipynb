{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1774,
     "status": "ok",
     "timestamp": 1749362976189,
     "user": {
      "displayName": "SeungMin Baek",
      "userId": "13244712800838483916"
     },
     "user_tz": -540
    },
    "id": "QR4Made_AH4J",
    "outputId": "525cd18b-2fbd-4918-e01c-c6bbaa79c823"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baek2\\AppData\\Local\\Temp\\ipykernel_37452\\804421021.py:3: DeprecationWarning: Importing from the 'elasticsearch.client' module is deprecated. Instead use 'elasticsearch' module for importing the client.\n",
      "  from elasticsearch.client import MlClient\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.client import MlClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28325,
     "status": "ok",
     "timestamp": 1749363008474,
     "user": {
      "displayName": "SeungMin Baek",
      "userId": "13244712800838483916"
     },
     "user_tz": -540
    },
    "id": "nf8xUTwdAIqL",
    "outputId": "d82f0be4-2a59-4805-ae30-278c8a4317b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'instance-0000000000', 'cluster_name': 'a30a7c8ef0bf435a9d350006622225d8', 'cluster_uuid': 'Y2Ofhp1kTYe--9mTVeNzVQ', 'version': {'number': '9.2.1', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '4ad0ef0e98a2e72fafbd79a19fa5cae2f026117d', 'build_date': '2025-11-06T22:07:39.673130621Z', 'build_snapshot': False, 'lucene_version': '10.3.1', 'minimum_wire_compatibility_version': '8.19.0', 'minimum_index_compatibility_version': '8.0.0'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "# ES 클라이언트 정의\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "url = os.getenv(\"ELASTIC_CLOUD_URL\")\n",
    "api_id = os.getenv(\"ELASTIC_API_ID\")\n",
    "api_key = os.getenv(\"ELASTIC_API_KEY\")\n",
    "es_model_id = os.getenv(\"ELASTIC_MODEL_ID\")\n",
    "\n",
    "\n",
    "client = Elasticsearch(\n",
    "    url,\n",
    "    api_key=(api_id, api_key)\n",
    ")\n",
    "\n",
    "print(client.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from typing import List, Dict\n",
    "\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. DATA > CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    단일 PDF 파일에서 모든 페이지 텍스트를 추출하는 함수.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    texts = []\n",
    "\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            texts.append(page_text)\n",
    "\n",
    "    return \"\\n\".join(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_txt(txt_path: str) -> str:\n",
    "    \"\"\"\n",
    "    단일 텍스트(.txt) 파일에서 텍스트를 읽어오는 함수.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        # utf-8로 읽기 실패 시, 한글 윈도우 기본 인코딩인 cp949로 시도\n",
    "        try:\n",
    "            with open(txt_path, 'r', encoding='cp949') as f:\n",
    "                return f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {txt_path}: {e}\")\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunk_text(\n",
    "    text: str,\n",
    "    max_chars: int = 1000,\n",
    "    overlap: int = 200\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    긴 텍스트를 max_chars 기준으로 잘라 chunk 리스트를 반환.\n",
    "    overlap만큼 앞 chunk와 겹치게 슬라이딩 윈도우 형태로 자름.\n",
    "    \"\"\"\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "\n",
    "    while start < text_length:\n",
    "        end = start + max_chars\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "        start = end - overlap\n",
    "        \n",
    "        if start < 0:\n",
    "            start = 0\n",
    "        \n",
    "        if start >= text_length:\n",
    "            break\n",
    "            \n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_document_chunk_dataframe(\n",
    "    folder_path: str,\n",
    "    max_chars: int = 1000,\n",
    "    overlap: int = 200\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    주어진 폴더에서 PDF와 TXT 파일을 읽고 DataFrame 생성.\n",
    "    \"\"\"\n",
    "    pdf_files = glob.glob(os.path.join(folder_path, \"*.pdf\"))\n",
    "    txt_files = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
    "    all_files = pdf_files + txt_files\n",
    "    \n",
    "    records: List[Dict] = []\n",
    "\n",
    "    for file_path in all_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        _, ext = os.path.splitext(filename)\n",
    "        ext = ext.lower()\n",
    "        \n",
    "        print(f\"Processing: {filename}\")\n",
    "\n",
    "        full_text = \"\"\n",
    "        if ext == '.pdf':\n",
    "            full_text = extract_text_from_pdf(file_path)\n",
    "        elif ext == '.txt':\n",
    "            full_text = extract_text_from_txt(file_path)\n",
    "        \n",
    "        if not full_text.strip():\n",
    "            continue\n",
    "\n",
    "        chunks = chunk_text(full_text, max_chars=max_chars, overlap=overlap)\n",
    "\n",
    "        for i, chunk in enumerate(chunks, start=1):\n",
    "            # chunk_text 맨 앞에 파일명을 명시적으로 추가\n",
    "            chunk_with_filename = f\"[{filename}] {chunk}\"\n",
    "\n",
    "            records.append(\n",
    "                {\n",
    "                    \"filename\": filename,\n",
    "                    \"extension\": ext,\n",
    "                    \"chunk_seq\": i,\n",
    "                    \"chunk_text\": chunk_with_filename, # 파일명이 포함된 텍스트 저장\n",
    "                }\n",
    "            )\n",
    "\n",
    "    df = pd.DataFrame(records, columns=[\"filename\", \"extension\", \"chunk_seq\", \"chunk_text\"])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf\n",
      "Processing: 02.3주차 검색개요 - 색인과 검색랭킹과 평가(part2).pdf\n",
      "Processing: 03.4주차 ElasticSearch.pdf\n",
      "Processing: 04.5주차 LLM 이해와 PromptEngineering 9월 30일.pdf\n",
      "Processing: 05.7주차 생성형AI 검색 기초 - 벡터검색과 RAG_1014.pdf\n",
      "Processing: 06-1.LangChain 기초_별첨.pdf\n",
      "Processing: 06.제10강 검색에이전트와 LangChain LangGraph.pdf\n",
      "Processing: 07.제11강 LangChain LangGraph를 이용한 AgenticRAG 개발.pdf\n",
      "Processing: 08.제12강 Multi-Hop .pdf\n",
      "Processing: 09.제13강 멀티모달 검색.pdf\n",
      "Processing: 07-1.11강 LangChain LangGraph를 이용한 AgenticRag개발_11월11일수업내용.txt\n",
      "Processing: 09-1.13강 멀티모달 검색_1125일수업내용.txt\n",
      "                               filename extension  chunk_seq  \\\n",
      "0  01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf      .pdf          1   \n",
      "1  01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf      .pdf          2   \n",
      "2  01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf      .pdf          3   \n",
      "3  01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf      .pdf          4   \n",
      "4  01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf      .pdf          5   \n",
      "\n",
      "                                          chunk_text  \n",
      "0  [01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf] 빅데이터 정보...  \n",
      "1  [01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf] (Vector...  \n",
      "2  [01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf]  관련(?) ...  \n",
      "3  [01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf]  등 의미없는...  \n",
      "4  [01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf] 는 검색키워드...  \n"
     ]
    }
   ],
   "source": [
    "# 1.1 폴더 내 PDF파일을 읽어서 데이터 프레임으로 저장\n",
    "\n",
    "folder_path = \"D:/workspace/대학원/25년도2학기/정보검색프로젝트/색인데이터\"\n",
    "\n",
    "max_chars = 1000\n",
    "overlap = 200\n",
    "\n",
    "df = build_document_chunk_dataframe(\n",
    "    folder_path=folder_path,\n",
    "    max_chars=max_chars,\n",
    "    overlap=overlap\n",
    "    )\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 데이터 프레임을 CSV로 저장\n",
    "\n",
    "df.to_csv(\"pdf_chunks.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "executionInfo": {
     "elapsed": 2942,
     "status": "ok",
     "timestamp": 1749363049035,
     "user": {
      "displayName": "SeungMin Baek",
      "userId": "13244712800838483916"
     },
     "user_tz": -540
    },
    "id": "PNJq0TO_AgQR",
    "outputId": "68dccc06-0f0c-4547-c21a-26c550f2d34b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>extension</th>\n",
       "      <th>chunk_seq</th>\n",
       "      <th>chunk_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>[01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf] (Vector...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>3</td>\n",
       "      <td>[01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf]  관련(?) ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               filename extension  chunk_seq  \\\n",
       "1  01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf      .pdf          2   \n",
       "2  01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf      .pdf          3   \n",
       "\n",
       "                                          chunk_text  \n",
       "1  [01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf] (Vector...  \n",
       "2  [01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf]  관련(?) ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class_info = pd.read_csv('./pdf_chunks.csv', encoding='utf-8-sig')\n",
    "class_info.loc[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>extension</th>\n",
       "      <th>chunk_seq</th>\n",
       "      <th>chunk_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>[01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf] 빅데이터 정보...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>[01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf] (Vector...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>3</td>\n",
       "      <td>[01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf]  관련(?) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>4</td>\n",
       "      <td>[01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf]  등 의미없는...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>5</td>\n",
       "      <td>[01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf] 는 검색키워드...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>09-1.13강 멀티모달 검색_1125일수업내용.txt</td>\n",
       "      <td>.txt</td>\n",
       "      <td>34</td>\n",
       "      <td>[09-1.13강 멀티모달 검색_1125일수업내용.txt] 해서 그와 관련된 뭘 찾...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>09-1.13강 멀티모달 검색_1125일수업내용.txt</td>\n",
       "      <td>.txt</td>\n",
       "      <td>35</td>\n",
       "      <td>[09-1.13강 멀티모달 검색_1125일수업내용.txt] 는 우리 벤치마크 데이터...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>09-1.13강 멀티모달 검색_1125일수업내용.txt</td>\n",
       "      <td>.txt</td>\n",
       "      <td>36</td>\n",
       "      <td>[09-1.13강 멀티모달 검색_1125일수업내용.txt] 에서 수행할 수 있도록 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>09-1.13강 멀티모달 검색_1125일수업내용.txt</td>\n",
       "      <td>.txt</td>\n",
       "      <td>37</td>\n",
       "      <td>[09-1.13강 멀티모달 검색_1125일수업내용.txt]  하는 거예요. 근데 여...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>09-1.13강 멀티모달 검색_1125일수업내용.txt</td>\n",
       "      <td>.txt</td>\n",
       "      <td>38</td>\n",
       "      <td>[09-1.13강 멀티모달 검색_1125일수업내용.txt] 에 허리에 대해서 인스트...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>350 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 filename extension  chunk_seq  \\\n",
       "0    01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf      .pdf          1   \n",
       "1    01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf      .pdf          2   \n",
       "2    01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf      .pdf          3   \n",
       "3    01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf      .pdf          4   \n",
       "4    01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf      .pdf          5   \n",
       "..                                    ...       ...        ...   \n",
       "345        09-1.13강 멀티모달 검색_1125일수업내용.txt      .txt         34   \n",
       "346        09-1.13강 멀티모달 검색_1125일수업내용.txt      .txt         35   \n",
       "347        09-1.13강 멀티모달 검색_1125일수업내용.txt      .txt         36   \n",
       "348        09-1.13강 멀티모달 검색_1125일수업내용.txt      .txt         37   \n",
       "349        09-1.13강 멀티모달 검색_1125일수업내용.txt      .txt         38   \n",
       "\n",
       "                                            chunk_text  \n",
       "0    [01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf] 빅데이터 정보...  \n",
       "1    [01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf] (Vector...  \n",
       "2    [01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf]  관련(?) ...  \n",
       "3    [01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf]  등 의미없는...  \n",
       "4    [01.2주차 검색개요 - 색인과 검색랭킹 모델(part1).pdf] 는 검색키워드...  \n",
       "..                                                 ...  \n",
       "345  [09-1.13강 멀티모달 검색_1125일수업내용.txt] 해서 그와 관련된 뭘 찾...  \n",
       "346  [09-1.13강 멀티모달 검색_1125일수업내용.txt] 는 우리 벤치마크 데이터...  \n",
       "347  [09-1.13강 멀티모달 검색_1125일수업내용.txt] 에서 수행할 수 있도록 ...  \n",
       "348  [09-1.13강 멀티모달 검색_1125일수업내용.txt]  하는 거예요. 근데 여...  \n",
       "349  [09-1.13강 멀티모달 검색_1125일수업내용.txt] 에 허리에 대해서 인스트...  \n",
       "\n",
       "[350 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ES 색인 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"class-info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.1 임베딩을 위한 ingest pipeline 추가 \n",
    "\n",
    "client.ingest.put_pipeline(\n",
    "    id=\"pipeline\",\n",
    "    processors=[\n",
    "        {\n",
    "            \"inference\": {\n",
    "                # embedding에 활용할 model_id 지정\n",
    "                \"model_id\": es_model_id,  \n",
    "\n",
    "                # embedding 대상 text를 chunk_text 필드로 지정\n",
    "                \"field_map\": {\"chunk_text\": \"text_field\"},\n",
    "                  \n",
    "                # embedding 결과를 chunk_embedding 필드에 저장\n",
    "                \"target_field\": \"chunk_embedding\", \n",
    "            }\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 색인을 위한 analyzer, mapping 구성, pipeline 연결 \n",
    "\n",
    "index_body = {\n",
    "    \"settings\": {\n",
    "        \"index.mapping.exclude_source_vectors\": False, \n",
    "        \"analysis\": {\n",
    "            \"tokenizer\": {\n",
    "                \"nori_tokenizer_custom\": {\n",
    "                    \"type\": \"nori_tokenizer\",\n",
    "                    \"decompound_mode\": \"mixed\"\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"korean_nori\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"nori_tokenizer_custom\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"index\": {\n",
    "        \"number_of_replicas\": \"1\",\n",
    "        \"number_of_shards\": \"1\",\n",
    "        \"default_pipeline\": \"pipeline\",          # 파이프라인 적용설정\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"filename\": {\n",
    "                \"type\": \"keyword\"\n",
    "            },\n",
    "            \"chunk_seq\": {\n",
    "                \"type\": \"integer\"\n",
    "            },\n",
    "            \"chunk_text\": {                      # SparseSearch 적용을 위한 설정\n",
    "                \"type\": \"text\",                 \n",
    "                \"analyzer\": \"korean_nori\"  \n",
    "            },\n",
    "            \"chunk_embedding.predicted_value\": { # DenseSearch 적용을 위한 설정\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 768,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if client.indices.exists(index=index_name):\n",
    "    client.indices.delete(index=index_name)\n",
    "    print(f\"Deleted existing index: {index_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 인덱스 생성\n",
    "response = client.indices.create(\n",
    "    index=index_name,\n",
    "    body=index_body\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 246590,
     "status": "ok",
     "timestamp": 1749366388134,
     "user": {
      "displayName": "SeungMin Baek",
      "userId": "13244712800838483916"
     },
     "user_tz": -540
    },
    "id": "e3956274",
    "outputId": "0b39858e-41a4-456b-bb08-edad56960626"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully indexed document with index 0\n",
      "Successfully indexed document with index 30\n",
      "Successfully indexed document with index 60\n",
      "Successfully indexed document with index 90\n",
      "Successfully indexed document with index 120\n",
      "Successfully indexed document with index 150\n",
      "Successfully indexed document with index 180\n",
      "Successfully indexed document with index 210\n",
      "Successfully indexed document with index 240\n",
      "Successfully indexed document with index 270\n",
      "Successfully indexed document with index 300\n",
      "Successfully indexed document with index 330\n"
     ]
    }
   ],
   "source": [
    "# 2.4 데이터 색인\n",
    "\n",
    "indexed_results = []\n",
    "\n",
    "for index, row in class_info.iterrows():\n",
    "    doc_source = {\n",
    "        \"filename\": row['filename'] if pd.notna(row['filename']) else None,\n",
    "        \"chunk_seq\": row['chunk_seq'] if pd.notna(row['chunk_seq']) else None,\n",
    "        \"chunk_text\": row['chunk_text'] if pd.notna(row['chunk_text']) else None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = client.index(index=\"class-info\", document=doc_source, id=str(index))\n",
    "        indexed_results.append({'index': index, 'status': 'success', 'response': response})\n",
    "        if index % 30 == 0:\n",
    "            print(f\"Successfully indexed document with index {index}\")\n",
    "    except Exception as e:\n",
    "        indexed_results.append({'index': index, 'status': 'failed', 'error': str(e)})\n",
    "        print(f\"Failed to index document with index {index}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMS+C+jMgnvvqKumRbPkRn+",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
